#!/usr/bin/env python3
"""
policies.py

Explanation policies + utility helpers for the simulation.

Key design points:
- Explanation attributes are binary families: modality, scope, detail.
- Users have latent preferences theta_true[f] = P(prefer value=1 for family f).
- Feedback is a noisy "like" signal generated by config.sample_feedback().
- Policies choose (explain?, attrs) given an EpisodeContext.

Policies implemented:
  * NE: NoExplainPolicy        (never explain)
  * AE: AlwaysExplainPolicy    (always maximal explanation)
  * NO: NormOnlyPolicy         (sample attributes from normative priors)
  * PO: PreferenceOnlyPolicy   (sample attributes from population preference)
  * BA: BayesianAdaptivePolicy (learn per-user preferences; mix with norms)
  * OR: OraclePolicy           (knows true preferences; used for regret only)
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, Optional
import random

from config import (
    ATTRIBUTE_FAMILIES,
    NORM_PHI,
    UserType,
    expected_like_prob,
    P_LIKE_MATCH,
    P_LIKE_MISMATCH,
)

# ---------------------------------------------------------------------------
# Episode context and utility helpers
# ---------------------------------------------------------------------------

@dataclass
class EpisodeContext:
    """Context visible to policies at each episode."""
    response_type: str
    event_salient: bool


def explanation_cost(attrs: Dict[str, int]) -> float:
    """
    Simple (interpretable) explanation cost model.

    Base cost:
      - Any explanation costs 1 unit.
      - Detailed explanations add +1 unit.

    (If you want modality/scope to contribute to cost, extend this function.)
    """
    cost = 1.0
    if int(attrs.get("detail", 0)) == 1:
        cost += 1.0
    return cost


def curiosity_penalty(explained: bool, ctx: EpisodeContext, user_type: UserType) -> float:
    """
    Curiosity / unmet-information penalty when the robot stays silent in a salient episode.

    If explained OR not salient -> 0.
    Otherwise scale penalty by user's preference for detail (proxy for "cares about explanations").
    """
    if explained or (not ctx.event_salient):
        return 0.0
    return 2.0 * float(user_type.theta_true["detail"])


# ---------------------------------------------------------------------------
# Base policy interface
# ---------------------------------------------------------------------------

class ExplanationPolicy:
    """Abstract-ish interface for policies."""
    name: str

    def reset_user(self) -> None:
        """Reset per-user state at the start of a run."""
        return

    def choose(self, ctx: EpisodeContext, rng: random.Random):
        """Return (explain_flag, attrs_dict)."""
        raise NotImplementedError

    def update_from_feedback(
        self,
        feedback: Dict[str, int],
        chosen_attrs: Optional[Dict[str, int]] = None,
    ) -> None:
        """Update internal state given feedback. Stateless policies ignore."""
        return


# ---------------------------------------------------------------------------
# Baselines
# ---------------------------------------------------------------------------

class NoExplainPolicy(ExplanationPolicy):
    def __init__(self) -> None:
        self.name = "NE"

    def choose(self, ctx: EpisodeContext, rng: random.Random):
        return False, {fam: 0 for fam in ATTRIBUTE_FAMILIES}


class AlwaysExplainPolicy(ExplanationPolicy):
    def __init__(self) -> None:
        self.name = "AE"

    def choose(self, ctx: EpisodeContext, rng: random.Random):
        return True, {"modality": 1, "scope": 1, "detail": 1}


class NormOnlyPolicy(ExplanationPolicy):
    """Norm-only: explains when salient, attributes drawn from NORM_PHI."""

    def __init__(self) -> None:
        self.name = "NO"

    def choose(self, ctx: EpisodeContext, rng: random.Random):
        explain = bool(ctx.event_salient)
        attrs: Dict[str, int] = {}
        for fam in ATTRIBUTE_FAMILIES:
            p1 = float(NORM_PHI[fam][ctx.response_type])
            attrs[fam] = 1 if rng.random() < p1 else 0
        return explain, attrs


class PreferenceOnlyPolicy(ExplanationPolicy):
    """Population-preference-only: explains when salient, attributes from population_theta."""

    def __init__(self, population_theta: Dict[str, float]) -> None:
        self.name = "PO"
        self.population_theta = {k: float(v) for k, v in population_theta.items()}

    def choose(self, ctx: EpisodeContext, rng: random.Random):
        explain = bool(ctx.event_salient)
        attrs: Dict[str, int] = {}
        for fam in ATTRIBUTE_FAMILIES:
            p1 = float(self.population_theta[fam])
            attrs[fam] = 1 if rng.random() < p1 else 0
        return explain, attrs


# ---------------------------------------------------------------------------
# Bayesian Adaptive (BA): Beta belief + alpha-mixing with norms
# ---------------------------------------------------------------------------

@dataclass
class BayesianAdaptivePolicy(ExplanationPolicy):
    """
    BA: maintains per-user Beta beliefs over theta_f = P(prefer value=1 for family f),
    updates from feedback, and chooses the (explain?, attrs) that maximizes expected utility.

    alpha_mix in [0,1] controls balance between norms and learned preferences:
        p_pref1 = alpha_mix * phi_f(rt) + (1-alpha_mix) * E[theta_f]
    """
    alpha_mix: float = 0.5
    name: str = "BA"
    alpha_beta: Dict[str, list] = field(default_factory=dict)
    user_type: Optional[UserType] = None  # needed for curiosity penalty when staying silent

    def __post_init__(self) -> None:
        if not self.alpha_beta:
            self.alpha_beta = {fam: [1.0, 1.0] for fam in ATTRIBUTE_FAMILIES}

    def reset_user(self) -> None:
        self.alpha_beta = {fam: [1.0, 1.0] for fam in ATTRIBUTE_FAMILIES}

    def posterior_mean(self, fam: str) -> float:
        a, b = self.alpha_beta[fam]
        return float(a) / float(a + b)

    def mixed_pref1(self, fam: str, response_type: str) -> float:
        """Return p_pref1(fam, rt) according to the mixing equation (paper Eq. 3)."""
        phi = float(NORM_PHI[fam][response_type])
        theta_hat = float(self.posterior_mean(fam))
        return float(self.alpha_mix) * phi + (1.0 - float(self.alpha_mix)) * theta_hat

    def expected_attr_utility(self, attrs: Dict[str, int], ctx: EpisodeContext) -> float:
        """Expected utility for explain=True with a given attribute config."""
        reward = 0.0
        for fam in ATTRIBUTE_FAMILIES:
            p_pref1 = self.mixed_pref1(fam, ctx.response_type)
            reward += expected_like_prob(p_pref1, int(attrs[fam]), P_LIKE_MATCH, P_LIKE_MISMATCH)
        return reward - explanation_cost(attrs)

    def choose(self, ctx: EpisodeContext, rng: random.Random):
        # Option 1: stay silent
        if self.user_type is None:
            score_no = 0.0
        else:
            score_no = -curiosity_penalty(False, ctx, self.user_type)

        best_score = score_no
        best_explain = False
        best_attrs = {fam: 0 for fam in ATTRIBUTE_FAMILIES}

        # Option 2: explain with any of the 2^3 configs
        for m in (0, 1):
            for s in (0, 1):
                for d in (0, 1):
                    attrs = {"modality": m, "scope": s, "detail": d}
                    score = self.expected_attr_utility(attrs, ctx)
                    if score > best_score:
                        best_score = score
                        best_explain = True
                        best_attrs = attrs

        return best_explain, best_attrs

    def update_from_feedback(
        self,
        feedback: Dict[str, int],
        chosen_attrs: Optional[Dict[str, int]] = None,
    ) -> None:
        """
        Heuristic Beta update that is directionally consistent:

        - If user liked (y=1): push belief toward the chosen value x
        - If user disliked (y=0): push belief away from x

        For x in {0,1}:
          if x==1: a += y ; b += (1-y)
          if x==0: a += (1-y) ; b += y
        """
        if chosen_attrs is None:
            return

        for fam, y in feedback.items():
            x = int(chosen_attrs.get(fam, 0))
            a, b = self.alpha_beta[fam]
            if x == 1:
                a += y
                b += (1 - y)
            else:
                a += (1 - y)
                b += y
            self.alpha_beta[fam] = [a, b]


# ---------------------------------------------------------------------------
# Oracle (used only for regret computation)
# ---------------------------------------------------------------------------

@dataclass
class OraclePolicy(ExplanationPolicy):
    user_type: UserType
    name: str = "OR"

    def choose(self, ctx: EpisodeContext, rng: random.Random):
        # Option 1: stay silent
        best_score = -curiosity_penalty(False, ctx, self.user_type)
        best_explain = False
        best_attrs = {fam: 0 for fam in ATTRIBUTE_FAMILIES}

        # Option 2: explain with any config
        for m in (0, 1):
            for s in (0, 1):
                for d in (0, 1):
                    attrs = {"modality": m, "scope": s, "detail": d}
                    score = self.expected_attr_utility(attrs)  # explain=True
                    if score > best_score:
                        best_score = score
                        best_explain = True
                        best_attrs = attrs

        return best_explain, best_attrs

    def expected_attr_utility(self, attrs: Dict[str, int]) -> float:
        reward = 0.0
        for fam in ATTRIBUTE_FAMILIES:
            theta_true = float(self.user_type.theta_true[fam])
            reward += expected_like_prob(theta_true, int(attrs[fam]), P_LIKE_MATCH, P_LIKE_MISMATCH)
        return reward - explanation_cost(attrs)
